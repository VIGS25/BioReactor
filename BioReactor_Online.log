Finished Loading File: 64-probe1.xlsx 
Finished Loading File: 65-probe2.xlsx 
Finished Loading File: 66-probe3.xlsx 
Finished Loading File: 67-probe1.xlsx 
Finished Loading File: 68-probe2.xlsx 
Finished Loading File: 69-probe3.xlsx 
Finished Loading File: 71-probe2.xlsx 
Finished Loading File: 72-probe3.xlsx 
Finished Loading File: 73-probe1.xlsx 
Finished Loading File: 74-probe2.xlsx 
Finished Loading File: 75-probe3.xlsx 
Finished Loading File: 76-probe1.xlsx 
Finished Loading File: 78-probe3.xlsx 
Finished Loading File: 80-probe1.xlsx 
Finished Loading File: 81-probe2.xlsx 
Finished Loading File: 82-probe3.xlsx 
Finished Loading File: 83-probe1.xlsx 
Finished Loading File: 84-probe2.xlsx 
Finished Loading File: 85-probe3.xlsx 


Carrying out the analysis for variable Titer

The Train Files are: 

['71-probe2.xlsx', '72-probe3.xlsx', '81-probe2.xlsx', '82-probe3.xlsx', '65-probe2.xlsx', '66-probe3.xlsx', '68-probe2.xlsx', '74-probe2.xlsx', '69-probe3.xlsx', '75-probe3.xlsx', '76-probe1.xlsx']

 The test files are: 

['78-probe3.xlsx', '64-probe1.xlsx', '84-probe2.xlsx', '85-probe3.xlsx', '73-probe1.xlsx', '80-probe1.xlsx', '83-probe1.xlsx', '67-probe1.xlsx']

OFFLINE + INTERPOLATED DATA


 Cross Validation Fold No. 1
The best model is: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
             max_leaf_nodes=None, min_impurity_decrease=0.0,
             min_impurity_split=None, min_samples_leaf=1,
             min_samples_split=2, min_weight_fraction_leaf=0.0,
             n_estimators=100, presort='auto', random_state=None,
             subsample=1.0, verbose=0, warm_start=False)
The model has an rmse of 0.0772



 Cross Validation Fold No. 2
The best model is: SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
The model has an rmse of 0.1059



 Cross Validation Fold No. 3
The best model is: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,
       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1)
The model has an rmse of 0.0996



 Cross Validation Fold No. 4
The best model is: PLSRegression(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)
The model has an rmse of 0.0348



 Cross Validation Fold No. 5
The best model is: PLSRegression(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)
The model has an rmse of 0.1046



 Testing
The best model is: AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,
           max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           presort=False, random_state=None, splitter='best'),
         learning_rate=1.0, loss='linear', n_estimators=100,
         random_state=None)
The model has an rmse of 0.0911


